from keras.datasets import mnist
import matplotlib.pyplot as plt
(train_x,train_y),(test_x,test_y)=mnist.load_data()
print(train_x.shape)
train_x[1]
print(test_y.shape)
for i in range(9):
    plt.subplot(340 + 1 + i)
    plt.imshow(train_x[i],cmap= plt.get_cmap('gray'))
    
train_x = train_x.reshape(train_x.shape[0],28,28,1)
train_x.shape 

test_x = test_x.reshape(test_x.shape[0],28,28,1)
test_x.shape

import numpy as np

import keras
test_y = keras.utils.to_categorical(test_y)
train_y = keras.utils.to_categorical(train_y)

#normalizing train_X & test_x so that all of the inputs have the same scale
train_x = train_x.astype('float')
test_x = test_x.astype('float')
train_x = train_x/255.0
test_x = test_x/255.0

from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPool2D
from keras.layers import Dense, BatchNormalization
from keras.layers import Flatten
from keras.optimizers import SGD
def buildmodel():
    model = Sequential()
    model.add(Conv2D(32,kernel_size=3,activation='relu',input_shape=(28,28,1)))
    model.add(BatchNormalization())
    model.add(MaxPool2D(2)) 
    model.add(Flatten())
    model.add(Dense(100,activation='relu'))
    model.add(BatchNormalization())
    model.add(Dense(10,activation='softmax'))
    model.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])
    return model

#calculate crossvalscore
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from keras.wrappers.scikit_learn import KerasClassifier

#wrapping it in keras classifier
model1 = KerasClassifier(build_fn=buildmodel)
kfold = KFold(n_splits=5,shuffle=True,random_state=43)
cv = cross_val_score(model1,train_x,train_y,cv=kfold)
print(cv)

def model_evaluate(train_x,train_y,test_x,test_y):
    model = buildmodel()
    history = model1.fit(train_x,train_y,epochs=10,validation_data=(test_x,test_y))
    loss = history.history['loss']
    acc = history.history['acc']
    valloss = history.history['valloss']
    valacc = history.history['valacc']
    return loss,acc,valloss,valacc

def summary_diagnostics(loss,acc,valloss,valacc):
    plt.plot(loss)
    plt.plot(acc)
    plt.plot(valloss)
    plt.plot(valacc)

history = model1.fit(train_x,train_y,epochs=10,validation_data=(test_x,test_y))
loss = history.history['loss']
acc = history.history['acc']
valloss = history.history['valloss']
valacc = history.history['valacc']
plt.plot(loss)
plt.plot(acc)
